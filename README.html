<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<p align="center">
  <a href="https://github.com/chong-yao/naic/raw/main/README.pdf">
    <img src="https://img.shields.io/badge/PDF-Download This Writeup-blue?style=for-the-badge&amp;logo=adobeacrobatreader" alt="Download README PDF">
  </a>
</p>
<h1 id="naic-ai-technical-team-cantbyteus">NAIC (AI Technical) team: CantByteUs</h1>
<h2 id="by">By:</h2>
<p><strong>Ong Chong Yao <br>
Terrence Ong Jun Han <br>
Ng Tze Yang <br>
Chin Zhi Xian</strong></p>
<h2 id="data-collection--preparation">Data Collection &amp; Preparation</h2>
<ul>
<li>Scraped 2500~ images per class from the internet using the Bing and Google search engines.
<ul>
<li><a href="https://pypi.org/project/better-bing-image-downloader/">'better-bing-image-downloader' library in PyPi</a></li>
</ul>
</li>
<li>Converted the images to tensors.</li>
<li>Computed the tensor differences between images to find out the image similarities, thus removing duplicates effectively.
<ul>
<li><a href="https://github.com/elisemercury/Duplicate-Image-Finder">Duplicate Image Finder by elisemercury</a></li>
</ul>
</li>
<li>Manually filtered unrelated images out (e.g., images with a huge YouTube logo on it), then combined them with some images taken by ourselves.</li>
<li>Annotated a few of the kuih for segmentation using Label Studio; thus, we used our genuine original dataset, as the existing kuih datasets on dataset sharing platforms were not in segmentation format.</li>
<li>Included a variety of images:
<ul>
<li><strong>High and low-resolution images.</strong> Low-res images for the model to generalise better, and high-res for the attention layers to capture the minute detail.</li>
<li><strong>Varied lighting and angles</strong> (to counter that most of the photos online were taken in optimal lighting and framing to be appealing for promotions and advertisements).</li>
<li><strong>Partially eaten kuih</strong></li>
</ul>
</li>
</ul>
<p><em>Even Bing and Google search engines weren't really able to tell the difference between Kuih Lapis and Kek Lapis, and often got the two classes jumbled up too!</em></p>
<h2 id="dataset-annotation-pseudo-labelling---a-dataset-annotation--semi-supervised-training-method">Dataset Annotation: Pseudo Labelling - a dataset annotation &amp; semi-supervised training method</h2>
<p>For each class:</p>
<ol>
<li>Annotated 20-30 kuih per class based on feature complexity.</li>
<li>Trained a small YOLOv11-seg model to speed up annotation.</li>
<li>Used the model to annotate the remaining images, with manual verification.</li>
<li>Retrained a larger model with the combined manually annotated and model-annotated images.</li>
<li>Repeated the process until the dataset was complete.</li>
</ol>
<p><em>Attached image explains pseudo-labelling:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/pseudo-labelling.jpg" alt="alt text"></p>
<p>Paper:<code> Lee, Dong-Hyun. (2013). Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL). We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With De-noising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.</code></p>
<p>Eventually, we plateaued with a raw dataset of 98 images in each class that were perfectly annotated by the model for its respective class.</p>
<ol start="7">
<li>
<p>Split the per-class dataset into 90 images for training and 8 images for validation.</p>
</li>
<li>
<p>We then uploaded all 784 ((90 + 8) × 8 classes) images to Roboflow for augmentation and to artificially increase the dataset size. This effectively tripled the dataset size while applying augmentations.</p>
<p><em>HOWEVER</em>, we purposely left the hue and color adjustment augmentations out because they would mess with how the model interprets the images, and because kuih identification is very color-sensitive.</p>
</li>
<li>
<p>Added a few non-kuih related images with no labels into the training split to reduce False Positives.</p>
</li>
</ol>
<p>After all this, we also wrote a script to render all the segmentation annotations on top of the images and then place all of them into a grid to be neatly visualised.</p>
<p><em>Attached image below shows our rendered final validation split, the different mask colors representing the 8 different classes:</em><img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/dataset/val-viz.jpg" alt="alt text"></p>
<h2 id="model-development">Model Development</h2>
<ul>
<li>
<p>Tools Used:</p>
<ul>
<li>Our own computers for all training and inference.</li>
<li>CUDA for GPU-accelerated training.</li>
<li>PyTorch and the Ultralytics library.</li>
</ul>
</li>
<li>
<p>Initial Approach:</p>
<ul>
<li>Directly edited the 'yolo11seg.yaml' model configuration file to increase depth, width, and channel capacity.</li>
<li>Added more attention layers.</li>
</ul>
</li>
<li>
<p>Problem: Large models tend to overfit on small-to-medium-sized datasets.</p>
</li>
<li>
<p>Solution:</p>
<ul>
<li>Trained a YOLOv11x-seg model from scratch on the full COCO 2017 dataset to preconfigure weights and biases.</li>
<li>Fine-tuned the model on the smaller kuih dataset.</li>
</ul>
</li>
</ul>
<p>We normalised the exposure of test images before inference to get a more consistent light balance all over the image, giving the model less of a hard time. But that could be solved by training the model with images preprocessed to have different exposure levels.</p>
<h2 id="final-model-an-ensemble-approach">Final Model: An Ensemble Approach</h2>
<p>We chose an ensemble of a CNN segmentation model and a Vision Transformer (ViT) model.</p>
<h3 id="why-segmentation">Why Segmentation?</h3>
<p>The confusion matrix for the YOLOv11-cls (classification) models wasn't at all that impressive:</p>
<p><em>Attached image shows the confusion matrix for YOLOv11m-cls model on a 50-images per class dataset:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/confusion_matrix_cls.png" alt="alt text"></p>
<p><em><strong>&quot;A robust segmentation model inherently improves classification accuracy&quot;</strong></em> - It prioritises the core image (the kuih) and reduces distractions.</p>
<p>We started training segmentation models:
<em>Attached image shows training &amp; validation metrics for the YOLOv11x-seg model:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/seg-metrics.png" alt="alt text"></p>
<p><strong>Notice how the cls_loss plummeted after only a few epochs?</strong></p>
<p>True enough, the confusion matrix for the segmentation model was near-perfect.</p>
<p><em>Attached image shows the confusion matrix for YOLOv11x-seg model on an 8-images per class validation split:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/confusion_matrix_normalized_seg.png" alt="alt text"></p>
<h3 id="why-vision-transformer">Why Vision Transformer?</h3>
<p>Vision Transformers split the input image into patches, and then &quot;transform&quot; the patches into tokens (like words in LLMs).</p>
<p><em><strong>&quot;ViTs can capture relationships across the entire image in every layer&quot;</strong></em></p>
<p>Kuih may look visually similar (looking at kek lapis-kuih lapis &amp; kuih seri muka-kuih talam similarities) BUT they have different textures that cannot be easily identified when only looking at a certain part of the image.</p>
<ul>
<li>
<p>ViTs use a 'self-attention' mechanism to analyse the entire image, capturing textures and long-distance relationships between image regions. Thus, even if a kuih looks slightly different across images, the ViT can still recognise it based on learnt patterns.</p>
</li>
<li>
<p>Vision Transformers perform well and more efficiently (even surpassing CNNs) when pretrained on a large dataset.</p>
</li>
<li>
<p>They perform well when pretrained on large datasets. We used the 'eva02_base_patch14_224.mim_in22k' model pretrained on ImageNet 22k.</p>
<ul>
<li><a href="https://huggingface.co/timm/eva02_base_patch14_224.mim_in22k">eva02_base_patch14_224.mim_in22k by timm</a></li>
</ul>
</li>
</ul>
<p><em>Attached image shows an instance of training the ViT:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/cy-train-vit.png" alt="alt text"></p>
<p><strong>Notice how fast the model converges even in the first few epochs?</strong></p>
<p><em>Attached image below shows a separate instance of training the ViT:</em>
<img src="file:///c:/Users/ochon/OneDrive/Documents/2025/National AI Competition/media/terr-train-vit.jpg" alt="alt text"></p>
<p><strong>ViTs plateau very fast</strong> (look at epoch 17). ViTs' fast convergence tends to bring the risk of overfitting too; thus, we had to be careful and save the model at every epoch.</p>
<h2 id="final-output-combining-cnn-and-vit">Final Output: Combining CNN and ViT</h2>
<p><strong>This is where we combined the outputs of the CNN and ViT to give an output.</strong></p>
<p>A soft voting method determines the final combined output of both models.</p>
<p>If the models agree, that class is chosen.</p>
<p>If they disagree, the class with the highest confidence from either model is chosen.</p>
<p>This hybrid approach outperformed solo models in classification tests.</p>

</body>
</html>
