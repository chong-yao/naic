{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NeFpCr0HIkzt"
      },
      "outputs": [],
      "source": [
        "# Ensure you have a CUDA-compatible version of PyTorch installed.\n",
        "!pip install -q ultralytics timm opencv-python Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezrdTSBSK4tB"
      },
      "source": [
        "# Load CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v9KGUYRtInXF"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "cnn = YOLO(\"models/cnn.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy69EVJRlijd"
      },
      "source": [
        "# Load ViT model and move to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uccgPddWlije"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ochon\\.conda\\envs\\naic\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Eva(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (rope): RotaryEmbeddingCat()\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x EvaBlock(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): EvaAttention(\n",
              "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): SwiGLU(\n",
              "        (fc1_g): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (fc1_x): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (act): SiLU()\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): Identity()\n",
              "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import timm\n",
        "import torch\n",
        "\n",
        "# MODIFIED: Define the device to use (CUDA or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# MODIFIED: Create the model and move it to the defined device\n",
        "vit = timm.create_model('eva02_base_patch14_224.mim_in22k', pretrained=False, num_classes=8).to(device)\n",
        "\n",
        "# MODIFIED: Load the model weights, mapping them to the same device\n",
        "vit.load_state_dict(torch.load(\"models/vit.pth\", map_location=device))\n",
        "\n",
        "# MODIFIED: Set to evaluation mode (no .cpu() needed)\n",
        "vit.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6aed2csNlije"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ARD7OneK7m-"
      },
      "source": [
        "# Load labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4rxPynozK9bt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'Kek Lapis', 1: 'Kuih Kaswi Pandan', 2: 'Kuih Ketayap', 3: 'Kuih Lapis', 4: 'Kuih Seri Muka', 5: 'Kuih Talam', 6: 'Kuih Ubi Kayu', 7: 'Onde-Onde'}\n"
          ]
        }
      ],
      "source": [
        "with open('models/labels.txt') as f:\n",
        "    labels = {i: line.strip().split(' ', 1)[1] for i, line in enumerate(f)}\n",
        "\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4sL7oD6abcd"
      },
      "source": [
        "# NEW: Live Webcam Inference on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     69\u001b[39m font = cv2.FONT_HERSHEY_SIMPLEX\n\u001b[32m     70\u001b[39m cv2.putText(frame, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, (\u001b[32m10\u001b[39m, \u001b[32m30\u001b[39m), font, \u001b[32m1\u001b[39m, (\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m0\u001b[39m), \u001b[32m2\u001b[39m, cv2.LINE_AA)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWebcam Live Inference (CUDA)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Exit loop if 'q' is pressed\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv2.waitKey(\u001b[32m1\u001b[39m) & \u001b[32m0xFF\u001b[39m == \u001b[38;5;28mord\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochon\\.conda\\envs\\naic\\Lib\\site-packages\\ultralytics\\utils\\patches.py:85\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(winname, mat)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(winname: \u001b[38;5;28mstr\u001b[39m, mat: np.ndarray):\n\u001b[32m     68\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    Display an image in the specified window.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m \u001b[33;03m        >>> imshow(\"Example Window\", img)  # Display the image\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43m_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43municode_escape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 8\n",
        "\n",
        "# Start webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam.\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the webcam\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to capture frame.\")\n",
        "        break\n",
        "\n",
        "    # --- 1. CNN Prediction (on GPU) ---\n",
        "    # YOLO automatically uses the GPU if available\n",
        "    cnn_results = cnn(frame, verbose=False)\n",
        "    cnn_pred_classes = cnn_results[0].boxes.cls.tolist()\n",
        "\n",
        "    if len(cnn_pred_classes) > 0:\n",
        "        cnn_counts = Counter(cnn_pred_classes)\n",
        "        cnn_probs = [cnn_counts.get(i, 0) for i in range(num_classes)]\n",
        "        cnn_probs = [p / sum(cnn_probs) for p in cnn_probs]\n",
        "        cnn_predicted_class_idx = int(max(cnn_counts, key=cnn_counts.get))\n",
        "    else:\n",
        "        cnn_probs = [0.0] * num_classes\n",
        "        cnn_predicted_class_idx = -1\n",
        "\n",
        "    # --- 2. ViT Prediction (on GPU) ---\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(image_rgb)\n",
        "    \n",
        "    # MODIFIED: Move the input tensor to the same device as the model\n",
        "    input_tensor = vit_transform(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vit_logits = vit(input_tensor)\n",
        "    \n",
        "    # Probabilities can be moved to CPU for subsequent logic\n",
        "    vit_probs_tensor = F.softmax(vit_logits, dim=1).squeeze(0).cpu()\n",
        "    vit_probs = vit_probs_tensor.tolist()\n",
        "    vit_predicted_class_idx = int(torch.argmax(vit_probs_tensor).item())\n",
        "\n",
        "    # --- 3. Combine Predictions (Ensemble) ---\n",
        "    final_probs = [(c + v) / 2 for c, v in zip(cnn_probs, vit_probs)]\n",
        "\n",
        "    if cnn_predicted_class_idx == vit_predicted_class_idx and cnn_predicted_class_idx != -1:\n",
        "        final_idx = cnn_predicted_class_idx\n",
        "    elif cnn_predicted_class_idx == -1 and vit_predicted_class_idx != -1:\n",
        "        final_idx = vit_predicted_class_idx\n",
        "    elif vit_predicted_class_idx == -1 and cnn_predicted_class_idx != -1:\n",
        "        final_idx = cnn_predicted_class_idx\n",
        "    elif cnn_predicted_class_idx != -1 and vit_predicted_class_idx != -1:\n",
        "        final_idx = int(torch.tensor(final_probs).argmax().item())\n",
        "    else:\n",
        "        final_idx = -1\n",
        "\n",
        "    final_label = labels.get(final_idx, 'Unknown')\n",
        "\n",
        "    # --- 4. Display the result on the frame ---\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(frame, f'Prediction: {final_label}', (10, 30), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    cv2.imshow('Webcam Live Inference (CUDA)', frame)\n",
        "\n",
        "    # Exit loop if 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# --- 5. Cleanup ---\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "naic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
