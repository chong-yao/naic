<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kuih Classification and Segmentation - CantByteUs</title>
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">
    <style>
        /* General Reset & Body Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            font-size: 16px; /* Base font size for rem units */
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Open Sans', sans-serif;
            background-color: #f8f9fa;
            color: #333;
            line-height: 1.6;
        }

        /* Main Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Header Section */
        .header {
            text-align: center;
            margin-bottom: 2.5rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2.5rem 1.5rem;
            border-radius: 12px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }

        .main-title {
            font-family: 'Montserrat', sans-serif;
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            text-shadow: 1px 1px 3px rgba(0,0,0,0.2);
        }

        .subtitle {
            font-size: 1.5rem;
            font-weight: 400;
            margin-bottom: 1.5rem;
            opacity: 0.95;
        }

        .team-info h3 {
            font-size: 1.25rem;
            margin-bottom: 0.5rem;
            color: #ffd700;
            font-family: 'Montserrat', sans-serif;
        }

        .team-members {
            font-size: 1.1rem;
            font-weight: 400;
        }

        /* Hero Image Section */
        .hero-section {
            margin-bottom: 2.5rem;
            text-align: center;
        }

        .hero-image {
            width: 100%;
            max-width: 800px;
            height: auto;
            border-radius: 15px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
            object-fit: cover;
        }

        /* Content Grid */
        .content-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 1.5rem;
        }

        /* Content Box Styling */
        .content-box {
            background: white;
            border-radius: 12px;
            padding: 1.5rem 2rem;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
            border-left: 5px solid;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            display: flex;
            flex-direction: column;
        }

        .content-box:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.12);
        }

        /* Color Coding for Sections */
        .data-collection { border-left-color: #4CAF50; }
        .pseudo-labelling { border-left-color: #2196F3; }
        .model-development { border-left-color: #FF9800; }
        .why-segmentation { border-left-color: #9C27B0; }
        .why-vit { border-left-color: #F44336; }
        .ensemble { 
            border-left-color: #607D8B;
            grid-column: 1 / -1; /* Span full width */
        }

        .section-title {
            font-family: 'Montserrat', sans-serif;
            font-size: 1.75rem;
            margin-bottom: 1rem;
            color: #2c3e50;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #ecf0f1;
        }

        .content-flex {
            display: flex;
            gap: 1.5rem;
            align-items: center;
        }

        .text-content {
            flex: 1;
        }

        .bullet-list {
            list-style: none;
            padding-left: 0;
        }

        .bullet-list li {
            font-size: 1rem;
            margin-bottom: 0.75rem;
            padding-left: 1.5rem;
            position: relative;
        }

        .bullet-list li::before {
            content: "▶";
            position: absolute;
            left: 0;
            color: #667eea;
            font-size: 0.8rem;
            top: 6px;
        }

        .bullet-list li strong {
            font-weight: 600;
            color: #2c3e50;
        }

        /* Diagram and Figure Styles */
        .diagram-container, .figure-container {
            margin-top: 1rem;
            text-align: center;
        }

        .diagram {
            max-width: 100%;
            height: auto;
            object-fit: contain;
            border-radius: 8px;
            background: #f8f9fa;
            padding: 0.5rem;
            border: 1px solid #ddd;
        }

        .figure-container figcaption {
            font-size: 0.9rem;
            color: #6c757d;
            margin-top: 0.5rem;
            font-style: italic;
        }

        /* Footer */
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding: 1.5rem;
            background: #34495e;
            color: white;
            border-radius: 8px;
            font-size: 1rem;
        }

        /* Responsive Media Queries */
        @media (max-width: 992px) {
            .content-grid {
                grid-template-columns: 1fr;
            }

            .ensemble {
                grid-column: 1; /* Reset span */
            }
        }

        @media (max-width: 768px) {
            html {
                font-size: 14px;
            }
            
            .container {
                padding: 1rem;
            }
            
            .main-title {
                font-size: 2.5rem;
            }
            
            .subtitle {
                font-size: 1.25rem;
            }

            .content-flex {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <header class="header">
            <h1 class="main-title">Kuih Classification and Segmentation</h1>
            <h2 class="subtitle">Using Ensemble Learning: CNN Segmentation + Vision Transformer</h2>
            <div class="team-info">
                <h3>NAIC (AI Technical) Team: CantByteUs</h3>
                <p class="team-members">Chin Zhi Xian • Ng Tze Yang • Ong Chong Yao • Terrence Ong Jun Han</p>
            </div>
        </header>

        <!-- Hero Image Section -->
        <section class="hero-section">
            <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/experience-artistry-traditional-malaysian-kuih-desserts-stunning-detail-drone-shot-captures-exquisite-beauty-348234429.webp?token=GHSAT0AAAAAADDUO7B3OFRC33QA4366IMQG2DSKGNA" alt="A vibrant assortment of Malaysian Kuih on a banana leaf" class="hero-image">
        </section>

        <!-- Main Content Grid -->
        <div class="content-grid">
            <!-- Data Collection & Preparation -->
            <section class="content-box data-collection">
                <h3 class="section-title">Data Collection & Preparation</h3>
                <ul class="bullet-list">
                    <li><strong>Scraped ~2500 images/class</strong> from Bing and Google.</li>
                    <li><strong>Removed duplicates</strong> by computing tensor differences between images.</li>
                    <li><strong>Manually filtered</strong> unrelated images and combined with original photos.</li>
                    <li><strong>Annotated original dataset</strong> for segmentation using Label Studio due to a lack of existing segmentation-formatted kuih datasets.</li>
                    <li><strong>Final Dataset:</strong> Plateaued at 98 perfectly annotated images per class, split into 90 for training and 8 for validation.</li>
                    <li><strong>Augmentation:</strong> Used Roboflow to triple the dataset size, excluding hue/color adjustments to preserve color-sensitive features.</li>
                </ul>
            </section>

            <!-- Pseudo Labelling -->
            <section class="content-box pseudo-labelling">
                <h3 class="section-title">Pseudo Labelling Method</h3>
                <div class="content-flex">
                    <div class="text-content">
                        <ul class="bullet-list">
                            <li><strong>Efficient Annotation:</strong> Used a semi-supervised technique to create the dataset efficiently.</li>
                            <li><strong>Initial Labelling:</strong> Manually annotated 20-30 complex kuih images per class.</li>
                            <li><strong>Iterative Process:</strong> Trained a small YOLO model to annotate remaining images, followed by manual verification, and retrained a larger model on the combined data.</li>
                        </ul>
                    </div>
                    <div class="diagram-container">
                        <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/Schematic-diagram-of-pseudo-labeling.png?token=GHSAT0AAAAAADDUO7B37LEEBGNQ36PAFJCO2DSKGAA" alt="A diagram showing the Pseudo Labelling process: Unlabeled Data -> Model Prediction -> Combined with Labeled Data -> Retrain Model" class="diagram">
                    </div>
                </div>
            </section>

            <!-- Model Development -->
            <section class="content-box model-development">
                <h3 class="section-title">Model Development</h3>
                <ul class="bullet-list">
                    <li><strong>Tools:</strong> PyTorch, Ultralytics, and CUDA for GPU acceleration.</li>
                    <li><strong>Initial Challenge:</strong> Large, modified YOLO models tended to overfit on the small, specialized kuih dataset.</li>
                    <li><strong>Solution:</strong> Used a pre-trained **YOLOv11x-seg** model (trained on COCO 2017) and fine-tuned it on our kuih dataset.</li>
                    <li><strong>Preprocessing:</strong> Normalized image exposure during inference for more consistent lighting and better model performance.</li>
                </ul>
            </section>

            <!-- Why Segmentation -->
            <section class="content-box why-segmentation">
                <h3 class="section-title">Why Segmentation?</h3>
                <ul class="bullet-list">
                    <li><strong>"Robust segmentation inherently improves classification accuracy."</strong></li>
                    <li><strong>Focus on the Object:</strong> Prioritizes the kuih itself, significantly reducing background noise and distractions.</li>
                    <li><strong>Impressive Results:</strong> The classification loss plummeted after only a few epochs of training.</li>
                    <li><strong>Near-Perfect Matrix:</strong> Achieved a near-perfect, clean confusion matrix, validating the segmentation-first approach.</li>
                </ul>
                 <figure class="figure-container">
                    <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/confusion_matrix_normalized_seg.png?token=GHSAT0AAAAAADDUO7B3WWX7KHVRO3QSRO3S2DSKFMQ" alt="A normalized confusion matrix showing high accuracy (most values along the diagonal are above 0.8) for the YOLOv11x-seg model across 8 classes of kuih." class="diagram">
                    <figcaption>Normalized confusion matrix for the YOLOv11x-seg model.</figcaption>
                </figure>
            </section>

            <!-- Why Vision Transformer -->
            <section class="content-box why-vit">
                <h3 class="section-title">Why Vision Transformer (ViT)?</h3>
                 <ul class="bullet-list">
                    <li><strong>Global Context:</strong> Splits images into patches and uses a self-attention mechanism to capture long-distance relationships across the entire image in every layer.</li>
                    <li><strong>Texture Analysis:</strong> Effectively addresses subtle visual similarities (e.g., Kek Lapis vs. Kuih Lapis) by analyzing texture and patterns.</li>
                    <li><strong>Fast Convergence:</strong> Pre-trained on the large ImageNet 22k dataset (`eva02_base_patch14_224.mim_in22k`), the ViT model converged extremely quickly on our data.</li>
                     <li><strong>Overfitting Risk:</strong> Fast convergence required careful saving at each epoch to select the best-performing model before it began to overfit.</li>
                </ul>
            </section>
            
            <!-- Ensemble Approach -->
            <section class="content-box ensemble">
                <h3 class="section-title">Final Model: An Ensemble Approach</h3>
                <ul class="bullet-list">
                    <li><strong>Hybrid Power:</strong> Combines the strengths of the **CNN Segmentation model (YOLOv11x-seg)** and the **Vision Transformer (ViT)**.</li>
                    <li><strong>Soft Voting Method:</strong> A soft voting method determines the final classification.</li>
                    <li><strong>Decision Logic:</strong> If the models agree, that class is chosen. If they disagree, the class with the highest confidence score from either model is selected.</li>
                    <li><strong>Superior Performance:</strong> This hybrid approach significantly outperformed either of the solo models in classification tests, leading to a highly robust and accurate final model.</li>
                </ul>
            </section>
        </div>

        <!-- Footer -->
        <footer class="footer">
            <p>NAIC 2025 • AI Technical Competition • Team CantByteUs</p>
        </footer>
    </div>
</body>
</html>