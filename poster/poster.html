<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">
    <style>
    /* General Reset & Body Styles */
    * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }

    html {
        font-size: 19px; 
    }

    body {
        font-family: 'Open Sans', sans-serif;
        background-color: #f8f9fa;
        color: #333;
        line-height: 1.5;
        /* A2 Dimensions */
        width: 420mm;
        height: 594mm;
        margin: 0 auto;
        overflow: hidden;
        position: relative;
    }

    /* Main Container */
    .container {
        width: 100%;
        height: 100%;
        padding: 1rem;
        display: flex;
        flex-direction: column;
    }

    /* Header Section */
    .header {
        text-align: center;
        margin-bottom: 0rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 0rem;
        border-radius: 12px;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }

    .main-title {
        font-family: 'Montserrat', sans-serif;
        font-size: 2.5rem;
        font-weight: 700;
        margin-bottom: 0rem;
        text-shadow: 1px 1px 3px rgba(0,0,0,0.2);
    }

    .subtitle {
        font-size: 1.5rem;
        font-weight: 400;
        margin-bottom: 0rem;
        opacity: 0.95;
    }

    .team-info h3 {
        font-size: 1.4rem;
        margin-bottom: 0rem;
        color: #ffd700;
        font-family: 'Montserrat', sans-serif;
    }

    .team-members {
        font-size: 1.15rem;
        font-weight: 400;
    }

    /* Hero Image Section */
    .hero-section {
        margin-bottom: 0rem;
        text-align: center;
    }

    .hero-image {
        width: 100%;
        max-width: 100%;
        height: 120px;
        border-radius: 12px;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        object-fit: cover;
    }

    /* Content Grid */
    .content-grid {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 0.75rem;
        flex-grow: 1; 
    }

    /* Content Box Styling */
    .content-box {
        background: white;
        border-radius: 10px;
        padding: 0.5rem 1rem;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
        border-left: 5px solid;
        display: flex;
        flex-direction: column;
    }

    /* Color Coding for Sections */
    .data-collection { border-left-color: #4CAF50; }
    .pseudo-labelling { border-left-color: #2196F3; }
    .model-development { border-left-color: #FF9800; }
    .why-segmentation { 
        border-left-color: #9C27B0;
        grid-column: 1 / -1; /* MODIFIED: Span full width to accommodate horizontal layout */
    }
    .why-vit { border-left-color: #F44336; }
    .ensemble { 
        border-left-color: #607D8B;
        grid-column: 1 / -1;
    }

    .section-title {
        font-family: 'Montserrat', sans-serif;
        font-size: 1.5rem;
        margin-bottom: 0rem;
        color: #2c3e50;
        padding-bottom: 0rem;
        border-bottom: 2px solid #ecf0f1;
    }
    
    /* MODIFIED: Flex container for horizontal layouts */
    .content-flex {
        display: flex;
        flex-direction: row;
        align-items: flex-start;
        gap: 1rem;
        flex-grow: 1;
    }

    .text-content {
        flex: 1; 
    }
    
    .diagram-container {
       flex: 0.8;
       margin-top: 0;
    }

    /* MODIFIED: Specific sizing for the segmentation section's content */
    .why-segmentation .text-content {
        flex: 1;
        margin-top: 0rem;
    }

    .why-segmentation .figure-container {
        flex: 1.2; /* Give more space to the wide image */
        margin-top: 0;
    }

    .bullet-list {
        list-style: none;
        padding-left: 0;
    }

    .bullet-list li {
        font-size: 1.22rem; 
        margin-bottom: 0rem; 
        padding-left: 1rem;
        position: relative;
    }

    .bullet-list li::before {
        content: "▶";
        position: absolute;
        left: 0;
        color: #667eea;
        font-size: 0.8rem;
        top: 5px;
    }

    .bullet-list li strong {
        font-weight: 600;
        color: #2c3e50;
    }

    .diagram {
        max-width: 100%;
        height: auto;
        object-fit: contain;
        border-radius: 8px;
        background: #f8f9fa;
        padding: 0.5rem;
        border: 1px solid #ddd;
    }

    .figure-container {
        margin-top: 0rem; 
        text-align: center;
    }
    
    .figure-container figcaption {
        font-size: 0.9rem; 
        color: #6c757d;
        margin-top: 0rem;
        font-style: italic;
    }

    </style>
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <header class="header">
            <h2 class="main-title">Kuih Classification and Segmentation</h2>
            <h2 class="subtitle">Using Ensemble Learning: CNN Segmentation + Vision Transformer</h2>
            <div class="team-info">
                <h3>NAIC (AI Technical) Team: CantByteUs</h3>
                <p class="team-members">Chin Zhi Xian • Ng Tze Yang • Ong Chong Yao • Terrence Ong Jun Han</p>
            </div>
        </header>

        <!-- Hero Image Section -->
        <section class="hero-section">
            <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/experience-artistry-traditional-malaysian-kuih-desserts-stunning-detail-drone-shot-captures-exquisite-beauty-348234429.webp" alt="A vibrant assortment of Malaysian Kuih on a banana leaf" class="hero-image">
        </section>

        <!-- Main Content Grid -->
        <div class="content-grid">
            <!-- Data Collection & Preparation -->
            <section class="content-box data-collection">
                <h3 class="section-title">Data Collection & Preparation</h3>
                <ul class="bullet-list">
                    <li><strong>Scraped ~2500 images/class</strong> from Bing and Google.</li>
                    <li><strong>Removed duplicates</strong> by computing tensor differences between images.</li>
                    <li><strong>Manually filtered</strong> unrelated images and combined with original photos.</li>
                    <li><strong>Annotated original dataset</strong> for segmentation using Label Studio due to a lack of existing segmentation-formatted kuih datasets.</li>
                    <li><strong>Final Dataset:</strong> Plateaued at 98 perfectly annotated images per class, split into 90 for training and 8 for validation.</li>
                    <li><strong>Augmentation:</strong> Used Roboflow to triple the dataset size, excluding hue/color adjustments to preserve color-sensitive features.</li>
                </ul>
            </section>

            <!-- Pseudo Labelling -->
            <section class="content-box pseudo-labelling">
                <h3 class="section-title">Pseudo Labelling Method</h3>
                <div class="content-flex">
                    <div class="text-content">
                        <ul class="bullet-list">
                            <li><strong>Efficient Annotation:</strong> Used a semi-supervised technique to create the dataset efficiently.</li>
                            <li><strong>Initial Labelling:</strong> Manually annotated 20-30 complex kuih images per class.</li>
                            <li><strong>Iterative Process:</strong> Trained a small YOLO model to annotate remaining images, followed by manual verification, and retrained a larger model on the combined data.</li>
                        </ul>
                    </div>
                    <div class="diagram-container">
                        <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/Schematic-diagram-of-pseudo-labeling.png" alt="A diagram showing the Pseudo Labelling process" class="diagram">
                    </div>
                </div>
            </section>

            <!-- Model Development -->
            <section class="content-box model-development">
                <h3 class="section-title">Model Development</h3>
                <ul class="bullet-list">
                    <li><strong>Tools:</strong> PyTorch, Ultralytics, and CUDA for GPU acceleration.</li>
                    <li><strong>Initial Challenge:</strong> Large, modified YOLO models tended to overfit on the small, specialized kuih dataset.</li>
                    <li><strong>Solution:</strong> Used a pre-trained **YOLOv11x-seg** model and fine-tuned it on our kuih dataset.</li>
                    <li><strong>Preprocessing:</strong> Normalized image exposure during inference for better model performance.</li>
                </ul>
            </section>

            <!-- Why Vision Transformer (ViT)? -->
            <section class="content-box why-vit">
                <h3 class="section-title">Why Vision Transformer (ViT)?</h3>
                 <ul class="bullet-list">
                    <li><strong>Global Context:</strong> Splits images into patches and uses self-attention to capture long-distance relationships.</li>
                    <li><strong>Texture Analysis:</strong> Effectively addresses subtle visual similarities (e.g., Kek Lapis vs. Kuih Lapis).</li>
                    <li><strong>Fast Convergence:</strong> Pre-trained on ImageNet 22k, the ViT model converged extremely quickly on our data.</li>
                     <li><strong>Overfitting Risk:</strong> Required careful saving at each epoch to select the best model before it overfit.</li>
                </ul>
            </section>
            
            <!-- Why Segmentation? -->
            <section class="content-box why-segmentation">
                <h3 class="section-title">Why Segmentation?</h3>
                <!-- MODIFIED: Wrapper for horizontal layout -->
                <div class="content-flex">
                    <div class="text-content">
                        <ul class="bullet-list">
                            <li><strong>"Robust segmentation inherently improves classification accuracy."</strong></li>
                            <li><strong>Focus on the Object:</strong> Prioritizes the kuih itself, significantly reducing background noise and distractions.</li>
                            <li><strong>Impressive Results:</strong> The classification loss plummeted after only a few epochs of training.</li>
                            <li><strong>Near-Perfect Matrix:</strong> Achieved a near-perfect, clean confusion matrix, validating the segmentation-first approach.</li>
                        </ul>
                    </div>
                    <figure class="figure-container">
                        <img src="https://raw.githubusercontent.com/chong-yao/naic/refs/heads/main/media/confusion_matrix_normalized_seg.png" alt="A normalized confusion matrix showing high accuracy for the YOLOv11x-seg model" class="diagram">
                        <figcaption>Normalized confusion matrix for the YOLOv11x-seg model.</figcaption>
                    </figure>
                </div>
            </section>
            
            <!-- Ensemble Approach -->
            <section class="content-box ensemble">
                <h3 class="section-title">Final Model: An Ensemble Approach</h3>
                <ul class="bullet-list">
                    <li><strong>Hybrid Power:</strong> Combines the strengths of the **CNN Segmentation model (YOLOv11x-seg)** and the **Vision Transformer (ViT)**.</li>
                    <li><strong>Soft Voting Method:</strong> A soft voting method determines the final classification.</li>
                    <li><strong>Decision Logic:</strong> If models disagree, the class with the highest confidence score is selected.</li>
                    <li><strong>Superior Performance:</strong> This hybrid approach significantly outperformed either solo model, leading to a robust final model.</li>
                </ul>
            </section>
        </div>
    </div>
</body>
</html>